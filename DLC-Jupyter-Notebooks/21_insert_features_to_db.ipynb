{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import platform\n",
    "\n",
    "host = \"localhost\" if platform.system() == \"Windows\" else \"129.108.49.30\"\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"deeplabcut_db\", user=\"postgres\", password=\"1234\", host=host, port=\"5432\")\n",
    "\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Insert video file names (Run this from the computer where the videos are located)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the root of the project (i.e., where Jupyter Lab started)\n",
    "project_root = Path().resolve().parents[0]\n",
    "\n",
    "# Define base folder relative to root\n",
    "base_folder = project_root / \"data\" / \"ToyOnly\" / \"SplitVideos\"\n",
    "\n",
    "# Check path\n",
    "print(f\"Looking in: {base_folder}\")\n",
    "print(f\"Exists? {base_folder.exists()}\")\n",
    "\n",
    "# Collect video files\n",
    "video_files = list(base_folder.rglob(\"*.mp4\")) + list(base_folder.rglob(\"*.avi\"))\n",
    "\n",
    "# Convert to relative paths (relative to 'data')\n",
    "video_records = [(str(vf.relative_to(base_folder)),) for vf in video_files]\n",
    "\n",
    "# Insert into dlc_table\n",
    "insert_query = \"INSERT INTO dlc_table (video_name) VALUES (%s);\"\n",
    "\n",
    "# Avoid Duplicate Inserts\n",
    "cursor.execute(\"SELECT video_name FROM dlc_table;\")\n",
    "existing = set(row[0] for row in cursor.fetchall())\n",
    "\n",
    "# Filter only new videos\n",
    "new_records = [vr for vr in video_records if vr[0] not in existing]\n",
    "\n",
    "cursor.executemany(insert_query, new_records)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Code to Add All Subdirs of \"Python_scripts\" to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add Python_scripts to sys.path (parent of Extract_db_columns)\n",
    "scripts_dir = Path().resolve().parents[0] / \"Python_scripts\"\n",
    "sys.path.append(str(scripts_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Insert task, date_str, name, health, id_ in dlc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Extract_db_columns.parse_video_name\n",
    "importlib.reload(Extract_db_columns.parse_video_name)\n",
    "from Extract_db_columns.parse_video_name import parse_video_name\n",
    "\n",
    "cursor.execute(\"SELECT id, video_name FROM dlc_table WHERE video_name IS NOT NULL;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "updates = []\n",
    "\n",
    "for row in rows:\n",
    "    id_, video_name = row\n",
    "    task, date_str, name, health = parse_video_name(video_name)\n",
    "    updates.append((task, date_str, name, health, id_))\n",
    "\n",
    "# Update in batch\n",
    "cursor.executemany(\"\"\"\n",
    "UPDATE dlc_table\n",
    "SET task = %s, date = %s, name = %s, health = %s\n",
    "WHERE id = %s;\n",
    "\"\"\", updates)\n",
    "\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Insert num_frames, frame_rate, video_width, video_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Extract_db_columns.update_video_info\n",
    "\n",
    "importlib.reload(Extract_db_columns.update_video_info)\n",
    "\n",
    "# Call the function with subdirectories\n",
    "video_subdirs = ['FoodLight', 'FoodOnly', 'LightOnly', 'ToyOnly', 'ToyLight']\n",
    "Extract_db_columns.update_video_info.update_video_info_in_db(video_subdirs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Insert raw coordinates for bodyparts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Query dlc_table table to get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT id, video_name, frame_rate\n",
    "FROM dlc_table\n",
    "WHERE video_name IS NOT NULL AND frame_rate IS NOT NULL ORDER BY id;\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Extract_db_columns.find_csv_for_video\n",
    "import Extract_db_columns.parse_dlc_csv\n",
    "import Insert_to_featuretable.insert_dlc_arrays\n",
    "import Insert_to_featuretable.is_dlc_data_already_inserted\n",
    "\n",
    "importlib.reload(Extract_db_columns.find_csv_for_video)\n",
    "importlib.reload(Extract_db_columns.parse_dlc_csv)\n",
    "importlib.reload(Insert_to_featuretable.insert_dlc_arrays)\n",
    "importlib.reload(Insert_to_featuretable.is_dlc_data_already_inserted)\n",
    "\n",
    "from Extract_db_columns.find_csv_for_video import find_csv_for_video\n",
    "from Extract_db_columns.parse_dlc_csv import parse_dlc_csv\n",
    "from Insert_to_featuretable.insert_dlc_arrays import insert_dlc_arrays\n",
    "from Insert_to_featuretable.is_dlc_data_already_inserted import is_dlc_data_already_inserted\n",
    "\n",
    "\n",
    "# All bodyparts needs to be inserted\n",
    "bodyparts = ['Corner1', 'Corner2', 'Corner3', 'Corner4', 'Head', 'Neck', 'Tailbase']\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing DLC files\"):\n",
    "    video_id = row['id']\n",
    "    video_name = row['video_name']\n",
    "    frame_rate = row['frame_rate']\n",
    "    print(f\"{video_id}, {video_name}, {frame_rate}\")\n",
    "\n",
    "    # if is_dlc_data_already_inserted(conn, video_id):\n",
    "    #     print(f\"⏩ Skipping video_id {video_id}\")\n",
    "    #     log(f\"Skipped video_id {video_id} — already has data\")\n",
    "    #     continue\n",
    "\n",
    "    csv_path = find_csv_for_video(video_name)\n",
    "\n",
    "    if csv_path:\n",
    "        try:\n",
    "            rows = parse_dlc_csv(csv_path, frame_rate, bodyparts)\n",
    "            df_parsed = pd.DataFrame(rows)\n",
    "            insert_dlc_arrays(conn, video_id, df_parsed, bodyparts)\n",
    "\n",
    "            print(f\"✅ Inserted video_id {video_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error for video_id {video_id}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ CSV not found for {video_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyparts = ['Corner1', 'Corner2', 'Corner3', 'Corner4', 'Head', 'Neck', 'Tailbase']\n",
    "\n",
    "row = df.iloc[0]\n",
    "video_id = row['id']\n",
    "video_name = row['video_name']\n",
    "frame_rate = row['frame_rate']\n",
    "\n",
    "print(f\"Processing ID {video_id} — {video_name} @ {frame_rate} FPS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = find_csv_for_video(video_name)\n",
    "print(f\"CSV Path: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = parse_dlc_csv(csv_path, frame_rate, bodyparts)\n",
    "df_parsed = pd.DataFrame(rows)\n",
    "df_parsed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parsed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Extract_db_columns.find_csv_for_video\n",
    "import Extract_db_columns.parse_dlc_csv\n",
    "import Insert_to_featuretable.insert_dlc_arrays\n",
    "import Insert_to_featuretable.is_dlc_data_already_inserted\n",
    "\n",
    "importlib.reload(Extract_db_columns.find_csv_for_video)\n",
    "importlib.reload(Extract_db_columns.parse_dlc_csv)\n",
    "importlib.reload(Insert_to_featuretable.insert_dlc_arrays)\n",
    "importlib.reload(Insert_to_featuretable.is_dlc_data_already_inserted)\n",
    "\n",
    "from Extract_db_columns.find_csv_for_video import find_csv_for_video\n",
    "from Extract_db_columns.parse_dlc_csv import parse_dlc_csv\n",
    "from Insert_to_featuretable.insert_dlc_arrays import insert_dlc_arrays\n",
    "from Insert_to_featuretable.is_dlc_data_already_inserted import is_dlc_data_already_inserted\n",
    "\n",
    "insert_dlc_arrays(conn, video_id, df_parsed, bodyparts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_preview = pd.DataFrame(rows)\n",
    "df_preview.head()\n",
    "print(df_preview.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Completely Clear Table First (CAREFUL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"DELETE FROM dlc_files\")\n",
    "conn.commit()\n",
    "print(\"All entries deleted from dlc_files.\")\n",
    "# conn.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Insert csv files into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Folder with new CSVs\n",
    "csv_dir = r\"C:\\DeepLabCutProjects\\DLC-Atanu-2024-12-25\\Analyzed-videos-filtered\"\n",
    "csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Insert each CSV path\n",
    "for csv_file in csv_files:\n",
    "    coord_path = os.path.join(csv_dir, csv_file)\n",
    "\n",
    "    # Optional: check if already in DB to avoid duplicates\n",
    "    cursor.execute(\"SELECT 1 FROM dlc_files WHERE coord_path = %s\", (coord_path,))\n",
    "    exists = cursor.fetchone()\n",
    "    if exists:\n",
    "        print(f\"⚠️ Already in DB: {csv_file}\")\n",
    "        continue\n",
    "\n",
    "    cursor.execute(\"INSERT INTO dlc_files (coord_path) VALUES (%s)\", (coord_path,))\n",
    "    print(f\"✅ Inserted: {csv_file}\")\n",
    "\n",
    "conn.commit()\n",
    "# cursor.close()\n",
    "print(\"🎉 All new coord_paths uploaded with video_path = NULL.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Fix bad entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "fixes = {\n",
    "    'Eli.': 'Eli',\n",
    "    'Orelans': 'NewOrleans',\n",
    "    'London.': 'London'\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"deeplabcut_db\",\n",
    "    user=\"postgres\",\n",
    "    password=\"1234\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for old_name, corrected_name in fixes.items():\n",
    "    cursor.execute(\n",
    "        \"UPDATE dlc_files SET name = %s WHERE name = %s\",\n",
    "        (corrected_name, old_name)\n",
    "    )\n",
    "    print(f\"✔ Updated '{old_name}' → '{corrected_name}'\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"🎉 Name cleanup done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Insert maze number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "update_dlc_table.update_column(\"maze\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-DLC] *",
   "language": "python",
   "name": "conda-env-.conda-DLC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
